{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "356c77d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "สวัสดีครับ! ยินดีที่ได้รู้จักครับ ผมชื่อ Typhoon สร้างโดย SCB 10X เพื่อช่วยเหลือคุณอย่างมีประโยชน์ ปลอดภัย และซื่อสัตย์\n",
      "\n",
      "มีอะไรให้ผมช่วยวันนี้ครับ?\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "API_URL = \"http://localhost:5433\"\n",
    "MODEL_NAME = \"scb10x/typhoon2.1-gemma3-4b:latest\"\n",
    "\n",
    "def generate(prompt: str) -> str:\n",
    "    \"\"\"เรียก /api/generate เพื่อสร้างข้อความตอบกลับ\"\"\"\n",
    "    res = requests.post(\n",
    "        f\"{API_URL}/api/generate\",\n",
    "        json={\n",
    "            \"model\": MODEL_NAME,\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": False\n",
    "        }\n",
    "    )\n",
    "    res.raise_for_status()\n",
    "    return res.json()[\"response\"]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(generate(\"สวัสดี\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f306588c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ได้ทั้งหมด 14 chunks\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def load_and_chunk(directory: str, chunk_size: int = 800):\n",
    "    \"\"\"อ่านไฟล์ .txt ในโฟลเดอร์ แล้วแบ่งเป็นชิ้นละ chunk_size คำ\"\"\"\n",
    "    chunks = []\n",
    "    for fname in os.listdir(directory):\n",
    "        if not fname.endswith(\".txt\"):\n",
    "            continue\n",
    "        text = open(os.path.join(directory, fname), encoding=\"utf-8\").read()\n",
    "        words = text.split()\n",
    "        for i in range(0, len(words), chunk_size):\n",
    "            chunk = \" \".join(words[i : i + chunk_size])\n",
    "            # เก็บ metadata เบื้องต้นด้วย\n",
    "            chunks.append({\n",
    "                \"content\": chunk,\n",
    "                \"source\": fname\n",
    "            })\n",
    "    return chunks\n",
    "\n",
    "# usage\n",
    "directory = \"Raw-data-from-TTT\"\n",
    "list_of_chunks = load_and_chunk(directory)\n",
    "print(f\"ได้ทั้งหมด {len(list_of_chunks)} chunks\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "219f718b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\TTT_Trainee\\Project_1\\AI-Chatbot\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Arther\\.cache\\huggingface\\hub\\models--BAAI--bge-m3. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# โหลดโมเดล embedding\n",
    "embed_model = SentenceTransformer(\"BAAI/bge-m3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e4a9b205",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:16<00:00, 16.80s/it]\n"
     ]
    }
   ],
   "source": [
    "# แปลงเป็นเวกเตอร์ 768 มิติ\n",
    "texts = [c[\"content\"] for c in list_of_chunks]\n",
    "embeddings = embed_model.encode(texts, show_progress_bar=True)\n",
    "\n",
    "# เพิ่ม embedding ลงใน list_of_chunks\n",
    "for chunk, emb in zip(list_of_chunks, embeddings):\n",
    "    chunk[\"vector\"] = emb.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "57eff3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "conn = psycopg2.connect(\n",
    "    dbname=\"mydb\", user=\"admin\", password=\"1234\",\n",
    "    host=\"localhost\", port=\"5432\"\n",
    ")\n",
    "cur = conn.cursor()\n",
    "# สร้างตารางถ้ายังไม่มี\n",
    "cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS documents (\n",
    "        id SERIAL PRIMARY KEY,\n",
    "        content TEXT,\n",
    "        source TEXT,\n",
    "        embedding VECTOR(1024)\n",
    "    )\n",
    "\"\"\")\n",
    "conn.commit()\n",
    "\n",
    "# แทรกข้อมูล\n",
    "for c in list_of_chunks:\n",
    "    cur.execute(\"\"\"\n",
    "        INSERT INTO documents (content, source, embedding)\n",
    "        VALUES (%s, %s, %s)\n",
    "    \"\"\", (c[\"content\"], c[\"source\"], c[\"vector\"]))\n",
    "conn.commit()\n",
    "cur.close()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8838247",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embed_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 76\u001b[39m\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     75\u001b[39m     question = \u001b[33m\"\u001b[39m\u001b[33m5 ส คืออะไร\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m     answer = \u001b[43mgenerate_rag_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     77\u001b[39m     \u001b[38;5;28mprint\u001b[39m(answer)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 46\u001b[39m, in \u001b[36mgenerate_rag_response\u001b[39m\u001b[34m(query, top_k)\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_rag_response\u001b[39m(query: \u001b[38;5;28mstr\u001b[39m, top_k: \u001b[38;5;28mint\u001b[39m = \u001b[32m3\u001b[39m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m     45\u001b[39m     \u001b[38;5;66;03m# 2.1 สืบค้นเอกสารที่เกี่ยวข้อง\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     docs = \u001b[43mretrieve_similar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     48\u001b[39m     \u001b[38;5;66;03m# 2.2 ประกอบ context\u001b[39;00m\n\u001b[32m     49\u001b[39m     context = \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.join(\n\u001b[32m     50\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSource: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdoc[\u001b[33m'\u001b[39m\u001b[33msource\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mdoc[\u001b[33m'\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs\n\u001b[32m     51\u001b[39m     )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mretrieve_similar\u001b[39m\u001b[34m(query, top_k)\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[33;03mรับข้อความค้นหา (query) -> แปลงเป็นเวกเตอร์ -> \u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[33;03mSELECT เอกสารที่ embedding ใกล้เคียงที่สุด top_k ชิ้น\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# แปลง query เป็นเวกเตอร์\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m q_vec = \u001b[43membed_model\u001b[49m.encode([query], show_progress_bar=\u001b[38;5;28;01mFalse\u001b[39;00m)[\u001b[32m0\u001b[39m].tolist()\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# เชื่อมต่อฐานข้อมูล\u001b[39;00m\n\u001b[32m     14\u001b[39m conn = psycopg2.connect(\n\u001b[32m     15\u001b[39m     dbname=\u001b[33m\"\u001b[39m\u001b[33mmydb\u001b[39m\u001b[33m\"\u001b[39m, user=\u001b[33m\"\u001b[39m\u001b[33madmin\u001b[39m\u001b[33m\"\u001b[39m, password=\u001b[33m\"\u001b[39m\u001b[33m1234\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     16\u001b[39m     host=\u001b[33m\"\u001b[39m\u001b[33mlocalhost\u001b[39m\u001b[33m\"\u001b[39m, port=\u001b[33m\"\u001b[39m\u001b[33m5432\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     17\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'embed_model' is not defined"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "\n",
    "# 1. ฟังก์ชันสืบค้นเอกสารจาก Postgres ตามความใกล้เคียงของเวกเตอร์\n",
    "#จำนวนเอกสารมากสุดที่เราต้องการให้ SQL query ดึงกลับมาจากฐานข้อมูล\n",
    "def retrieve_similar(query: str, top_k: int = 3):\n",
    "    \"\"\"\n",
    "    รับข้อความค้นหา (query) -> แปลงเป็นเวกเตอร์ -> \n",
    "    SELECT เอกสารที่ embedding ใกล้เคียงที่สุด top_k ชิ้น\n",
    "    \"\"\"\n",
    "    # แปลง query เป็นเวกเตอร์\n",
    "    q_vec = embed_model.encode([query], show_progress_bar=False)[0].tolist()\n",
    "\n",
    "    # เชื่อมต่อฐานข้อมูล\n",
    "    conn = psycopg2.connect(\n",
    "        dbname=\"mydb\", user=\"admin\", password=\"1234\",\n",
    "        host=\"localhost\", port=\"5432\"\n",
    "    )\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    # ใช้ operator <-> ของ pgvector เพื่อคำนวณระยะ Euclidean\n",
    "    # ต้องแปลง array เป็น vector ด้วย ::vector\n",
    "    cur.execute(\"\"\"\n",
    "        SELECT content, source\n",
    "        FROM documents\n",
    "        ORDER BY embedding <-> (%s::vector)\n",
    "        LIMIT %s\n",
    "    \"\"\", (q_vec, top_k))\n",
    "\n",
    "    rows = cur.fetchall()\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "    # คืนผลเป็น list ของ dict\n",
    "    return [{\"content\": r[0], \"source\": r[1]} for r in rows]\n",
    "\n",
    "\n",
    "# 2. ฟังก์ชันรวม context และเรียก LLM (ตัวอย่างสำหรับ local API)\n",
    "import requests\n",
    "\n",
    "API_URL = \"http://localhost:5433\"       # แก้เป็น URL ของ LLM service\n",
    "MODEL_NAME = \"scb10x/typhoon2.1-gemma3-4b:latest\"\n",
    "\n",
    "#จะคืนผลลัพธ์ ... แถวแรกที่ embedding ใกล้เคียงที่สุดกับข้อความค้นหา\n",
    "def generate_rag_response(query: str, top_k: int = 3) -> str:\n",
    "    # 2.1 สืบค้นเอกสารที่เกี่ยวข้อง\n",
    "    docs = retrieve_similar(query, top_k)\n",
    "\n",
    "    # 2.2 ประกอบ context\n",
    "    context = \"\\n\\n\".join(\n",
    "        f\"Source: {doc['source']}\\n{doc['content']}\" for doc in docs\n",
    "    )\n",
    "    prompt = (\n",
    "    \"คุณคือผู้ช่วยอัจฉริยะชื่อ TTT-Assistant จากบริษัท ทีทีที บราเธอร์ส จำกัด \"\n",
    "    \"กรุณาตอบคำถามให้ละเอียดและครบถ้วนที่สุด โดยใช้ข้อมูลจาก context ที่มี\\n\"\n",
    "        f\"---\\nContext: {context}\\n---\"\n",
    "        \"\\n\"\n",
    "        f\"Question: {query}\"\n",
    "        \"\\nAnswer:\"\n",
    "    )\n",
    "\n",
    "    # 2.3 เรียก LLM API เพื่อสร้างคำตอบ\n",
    "    res = requests.post(\n",
    "        f\"{API_URL}/api/generate\",\n",
    "        json={\n",
    "            \"model\": MODEL_NAME,\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": False\n",
    "        }\n",
    "    )\n",
    "    res.raise_for_status()\n",
    "    return res.json()[\"response\"]\n",
    "\n",
    "# 3. วิธีใช้งาน\n",
    "if __name__ == \"__main__\":\n",
    "    question = \"5 ส คืออะไร\"\n",
    "    answer = generate_rag_response(question, top_k=3)\n",
    "    print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
