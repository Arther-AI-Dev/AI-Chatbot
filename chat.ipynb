{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "356c77d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "สวัสดีครับ! ยินดีที่ได้รู้จักครับ ผมชื่อ Typhoon สร้างโดย SCB 10X เพื่อให้ความช่วยเหลืออย่างมีประโยชน์ ปลอดภัย และซื่อสัตย์ครับ มีอะไรให้ผมช่วยไหมครับ?\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "API_URL = \"http://localhost:5433\"\n",
    "MODEL_NAME = \"scb10x/typhoon2.1-gemma3-4b:latest\"\n",
    "\n",
    "def generate(prompt: str) -> str:\n",
    "    \"\"\"เรียก /api/generate เพื่อสร้างข้อความตอบกลับ\"\"\"\n",
    "    res = requests.post(\n",
    "        f\"{API_URL}/api/generate\",\n",
    "        json={\n",
    "            \"model\": MODEL_NAME,\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": False\n",
    "        }\n",
    "    )\n",
    "    res.raise_for_status()\n",
    "    return res.json()[\"response\"]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(generate(\"สวัสดี\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fd4e883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[✓] Cleaned: BUSINESS LOOK Day.txt\n",
      "[✓] Cleaned: ที่อยู่สำนักงาน.txt\n",
      "[✓] Cleaned: บริการและสินค้า.txt\n",
      "[✓] Cleaned: พนักงานดีเด่น ประจำปี 2567.txt\n",
      "[✓] Cleaned: วันหยุดประจำปี พ.ศ. 2568.txt\n",
      "[✓] Cleaned: เกี่ยวกับ ทีทีที บราเธอร์ส.txt\n",
      "\n",
      "✅ Cleaning completed and saved to: preprocessdata\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "# === กำหนด path ===\n",
    "input_dir = 'data'\n",
    "output_dir = 'preprocessdata'\n",
    "\n",
    "# === ฟังก์ชัน clean ข้อความ ===\n",
    "def clean_text(text):\n",
    "    # ลบ space, tab, newline ซ้ำกัน\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'\\n+', '\\n', text)\n",
    "    text = re.sub(r'\\t+', ' ', text)\n",
    "    \n",
    "    # ลบอักขระพิเศษที่ไม่จำเป็น\n",
    "    text = re.sub(r'[^\\w\\sก-๙.,()/%&\\-–“”\"\\':]', '', text)\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "# === ตรวจสอบให้มีโฟลเดอร์ปลายทาง ===\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# === วนลูป clean ไฟล์ทั้งหมด ===\n",
    "for filename in os.listdir(input_dir):\n",
    "    if filename.endswith('.txt'):\n",
    "        input_path = os.path.join(input_dir, filename)\n",
    "        output_path = os.path.join(output_dir, filename)\n",
    "\n",
    "        with open(input_path, 'r', encoding='utf-8') as infile:\n",
    "            raw_text = infile.read()\n",
    "\n",
    "        cleaned_text = clean_text(raw_text)\n",
    "\n",
    "        with open(output_path, 'w', encoding='utf-8') as outfile:\n",
    "            outfile.write(cleaned_text)\n",
    "\n",
    "        print(f\"[✓] Cleaned: {filename}\")\n",
    "\n",
    "print(\"\\n✅ Cleaning completed and saved to:\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f306588c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ได้ทั้งหมด 6 chunks\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def load_and_chunk(directory: str, chunk_size: int = 3000):\n",
    "    \"\"\"อ่านไฟล์ .txt ในโฟลเดอร์ แล้วแบ่งเป็นชิ้นละ chunk_size คำ\"\"\"\n",
    "    chunks = []\n",
    "    for fname in os.listdir(directory):\n",
    "        if not fname.endswith(\".txt\"):\n",
    "            continue\n",
    "        text = open(os.path.join(directory, fname), encoding=\"utf-8\").read()\n",
    "        words = text.split()\n",
    "        for i in range(0, len(words), chunk_size):\n",
    "            chunk = \" \".join(words[i : i + chunk_size])\n",
    "            # เก็บ metadata เบื้องต้นด้วย\n",
    "            chunks.append({\n",
    "                \"content\": chunk,\n",
    "                \"source\": fname\n",
    "            })\n",
    "    return chunks\n",
    "\n",
    "# usage\n",
    "directory = \"preprocessdata\"\n",
    "list_of_chunks = load_and_chunk(directory)\n",
    "print(f\"ได้ทั้งหมด {len(list_of_chunks)} chunks\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9920d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# โหลดโมเดล embedding\n",
    "embed_model = SentenceTransformer(\"all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "219f718b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s]\n"
     ]
    }
   ],
   "source": [
    "# แปลงเป็นเวกเตอร์ 768 มิติ\n",
    "texts = [c[\"content\"] for c in list_of_chunks]\n",
    "embeddings = embed_model.encode(texts, show_progress_bar=True)\n",
    "\n",
    "# เพิ่ม embedding ลงใน list_of_chunks\n",
    "for chunk, emb in zip(list_of_chunks, embeddings):\n",
    "    chunk[\"vector\"] = emb.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "57eff3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "conn = psycopg2.connect(\n",
    "    dbname=\"mydb\", user=\"admin\", password=\"1234\",\n",
    "    host=\"localhost\", port=\"5432\"\n",
    ")\n",
    "cur = conn.cursor()\n",
    "# สร้างตารางถ้ายังไม่มี\n",
    "cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS documents (\n",
    "        id SERIAL PRIMARY KEY,\n",
    "        content TEXT,\n",
    "        source TEXT,\n",
    "        embedding VECTOR(768)\n",
    "    )\n",
    "\"\"\")\n",
    "conn.commit()\n",
    "\n",
    "# แทรกข้อมูล\n",
    "for c in list_of_chunks:\n",
    "    cur.execute(\"\"\"\n",
    "        INSERT INTO documents (content, source, embedding)\n",
    "        VALUES (%s, %s, %s)\n",
    "    \"\"\", (c[\"content\"], c[\"source\"], c[\"vector\"]))\n",
    "conn.commit()\n",
    "cur.close()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a8838247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "สวัสดีครับ ผม Typhoon ผู้ช่วยอัจฉริยะจาก SCB 10X ครับ\n",
      "\n",
      "TTT Brothers เป็นบริษัทที่เชี่ยวชาญด้านการให้บริการด้านการขนส่งโลจิสติกส์และการจัดการห่วงโซ่อุปทานครบวงจรครับ เราให้บริการหลากหลายรูปแบบ เช่น การขนส่งสินค้าทางบก ทางเรือ ทางอากาศ และบริการคลังสินค้า รวมถึงการจัดการระบบ Supply Chain ที่ครอบคลุม เพื่อให้ลูกค้าได้รับบริการที่มีประสิทธิภาพและตอบโจทย์ความต้องการได้อย่างครบวงจร\n",
      "\n",
      "หากมีคำถามเพิ่มเติมหรือต้องการข้อมูลเฉพาะด้านอื่นๆ สามารถสอบถามได้เลยนะครับ ยินดีให้บริการครับ\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# 2. ฟังก์ชันสืบค้นเอกสารจาก Postgres ตามความใกล้เคียงของเวกเตอร์\n",
    "def retrieve_similar(query: str, top_k: int = 5):\n",
    "    \"\"\"\n",
    "    รับข้อความค้นหา (query) -> แปลงเป็นเวกเตอร์ -> \n",
    "    SELECT เอกสารที่ embedding ใกล้เคียงที่สุด top_k ชิ้น\n",
    "    \"\"\"\n",
    "    # แปลง query เป็นเวกเตอร์\n",
    "    q_vec = embed_model.encode([query], show_progress_bar=False)[0].tolist()\n",
    "\n",
    "    # เชื่อมต่อฐานข้อมูล\n",
    "    conn = psycopg2.connect(\n",
    "        dbname=\"mydb\", user=\"admin\", password=\"1234\",\n",
    "        host=\"localhost\", port=\"5432\"\n",
    "    )\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    # ใช้ operator <-> ของ pgvector เพื่อคำนวณระยะ Euclidean\n",
    "    # ต้องแปลง array เป็น vector ด้วย ::vector\n",
    "    cur.execute(\"\"\"\n",
    "        SELECT content, source\n",
    "        FROM documents\n",
    "        ORDER BY embedding <-> (%s::vector)\n",
    "        LIMIT %s\n",
    "    \"\"\", (q_vec, top_k))\n",
    "\n",
    "    rows = cur.fetchall()\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "    # คืนผลเป็น list ของ dict\n",
    "    return [{\"content\": r[0], \"source\": r[1]} for r in rows]\n",
    "\n",
    "\n",
    "# 3. ฟังก์ชันรวม context และเรียก LLM (ตัวอย่างสำหรับ local API)\n",
    "import requests\n",
    "\n",
    "API_URL = \"http://localhost:5433\"       # แก้เป็น URL ของ LLM service\n",
    "MODEL_NAME = \"scb10x/typhoon2.1-gemma3-4b:latest\"\n",
    "\n",
    "def generate_rag_response(query: str, top_k: int = 5) -> str:\n",
    "    # 3.1 สืบค้นเอกสารที่เกี่ยวข้อง\n",
    "    docs = retrieve_similar(query, top_k)\n",
    "    \n",
    "    # ตรวจสอบความเกี่ยวข้องของเอกสารที่พบ\n",
    "    # หาก similarity score ต่ำเกินไป จะถือว่าไม่พบข้อมูลที่เกี่ยวข้อง\n",
    "    threshold = 0.3  # ปรับค่าตามความเหมาะสม\n",
    "    query_vec = embed_model.encode([query], show_progress_bar=False)[0]\n",
    "    docs_relevant = False\n",
    "    \n",
    "    if docs:\n",
    "        # ตรวจสอบ similarity กับเอกสารที่ได้\n",
    "        doc_vec = embed_model.encode([docs[0][\"content\"]], show_progress_bar=False)[0]\n",
    "        similarity = cosine_similarity([query_vec], [doc_vec])[0][0]\n",
    "        docs_relevant = similarity > threshold\n",
    "\n",
    "    if docs_relevant:\n",
    "        # มีเอกสารที่เกี่ยวข้อง ใช้ RAG\n",
    "        context = \"\\n\\n\".join(\n",
    "            f\"Source: {doc['source']}\\n{doc['content']}\" for doc in docs\n",
    "        )\n",
    "        prompt = (\n",
    "            \"คุณคือผู้ช่วยอัจฉริยะชื่อ TTT-Assistant ของบริษัท TTT Brothers กรุณาตอบคำถามโดยใช้ข้อมูลที่ให้มา\\n\"\n",
    "            f\"---\\nContext:\\n{context}\\n---\\n\"\n",
    "            f\"Question: {query}\\nAnswer:\"\n",
    "        )\n",
    "    else:\n",
    "        # ไม่พบเอกสารที่เกี่ยวข้อง ใช้ LLM ตอบคำถามทั่วไป\n",
    "        prompt = (\n",
    "            \"คุณคือผู้ช่วยอัจฉริยะชื่อ TTT-Assistant ของบริษัท TTT Brothers \"\n",
    "            \"ที่สามารถตอบคำถามได้อย่างสุภาพและเป็นมิตร\\n\\n\"\n",
    "            f\"Question: {query}\\nAnswer:\"\n",
    "        )\n",
    "\n",
    "    # เรียก LLM API\n",
    "    res = requests.post(\n",
    "        f\"{API_URL}/api/generate\",\n",
    "        json={\n",
    "            \"model\": MODEL_NAME,\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": False\n",
    "        }\n",
    "    )\n",
    "    res.raise_for_status()\n",
    "    return res.json()[\"response\"]\n",
    "\n",
    "# 4. วิธีใช้งาน\n",
    "if __name__ == \"__main__\":\n",
    "    question = \"บริษัท TTT Brothers เกี่ยวกับอะไร?\"\n",
    "    answer = generate_rag_response(question, top_k=3)\n",
    "    print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
