{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "356c77d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "สวัสดีครับ! ยินดีที่ได้รู้จักครับ ผมชื่อ Typhoon สร้างโดย SCB 10X เพื่อช่วยเหลือคุณอย่างมีประโยชน์ ปลอดภัย และซื่อสัตย์\n",
      "\n",
      "มีอะไรให้ผมช่วยวันนี้ครับ?\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "API_URL = \"http://localhost:5433\"\n",
    "MODEL_NAME = \"scb10x/typhoon2.1-gemma3-4b:latest\"\n",
    "\n",
    "def generate(prompt: str) -> str:\n",
    "    \"\"\"เรียก /api/generate เพื่อสร้างข้อความตอบกลับ\"\"\"\n",
    "    res = requests.post(\n",
    "        f\"{API_URL}/api/generate\",\n",
    "        json={\n",
    "            \"model\": MODEL_NAME,\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": False\n",
    "        }\n",
    "    )\n",
    "    res.raise_for_status()\n",
    "    return res.json()[\"response\"]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(generate(\"สวัสดี\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f306588c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ได้ทั้งหมด 14 chunks\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def load_and_chunk(directory: str, chunk_size: int = 800):\n",
    "    \"\"\"อ่านไฟล์ .txt ในโฟลเดอร์ แล้วแบ่งเป็นชิ้นละ chunk_size คำ\"\"\"\n",
    "    chunks = []\n",
    "    for fname in os.listdir(directory):\n",
    "        if not fname.endswith(\".txt\"):\n",
    "            continue\n",
    "        text = open(os.path.join(directory, fname), encoding=\"utf-8\").read()\n",
    "        words = text.split()\n",
    "        for i in range(0, len(words), chunk_size):\n",
    "            chunk = \" \".join(words[i : i + chunk_size])\n",
    "            # เก็บ metadata เบื้องต้นด้วย\n",
    "            chunks.append({\n",
    "                \"content\": chunk,\n",
    "                \"source\": fname\n",
    "            })\n",
    "    return chunks\n",
    "\n",
    "# usage\n",
    "directory = \"Raw-data-from-TTT\"\n",
    "list_of_chunks = load_and_chunk(directory)\n",
    "print(f\"ได้ทั้งหมด {len(list_of_chunks)} chunks\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "219f718b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\TTT_Trainee\\Project_1\\AI-Chatbot\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Arther\\.cache\\huggingface\\hub\\models--BAAI--bge-m3. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# โหลดโมเดล embedding\n",
    "embed_model = SentenceTransformer(\"BAAI/bge-m3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e4a9b205",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:16<00:00, 16.80s/it]\n"
     ]
    }
   ],
   "source": [
    "# แปลงเป็นเวกเตอร์ 768 มิติ\n",
    "texts = [c[\"content\"] for c in list_of_chunks]\n",
    "embeddings = embed_model.encode(texts, show_progress_bar=True)\n",
    "\n",
    "# เพิ่ม embedding ลงใน list_of_chunks\n",
    "for chunk, emb in zip(list_of_chunks, embeddings):\n",
    "    chunk[\"vector\"] = emb.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "57eff3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "conn = psycopg2.connect(\n",
    "    dbname=\"mydb\", user=\"admin\", password=\"1234\",\n",
    "    host=\"localhost\", port=\"5432\"\n",
    ")\n",
    "cur = conn.cursor()\n",
    "# สร้างตารางถ้ายังไม่มี\n",
    "cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS documents (\n",
    "        id SERIAL PRIMARY KEY,\n",
    "        content TEXT,\n",
    "        source TEXT,\n",
    "        embedding VECTOR(1024)\n",
    "    )\n",
    "\"\"\")\n",
    "conn.commit()\n",
    "\n",
    "# แทรกข้อมูล\n",
    "for c in list_of_chunks:\n",
    "    cur.execute(\"\"\"\n",
    "        INSERT INTO documents (content, source, embedding)\n",
    "        VALUES (%s, %s, %s)\n",
    "    \"\"\", (c[\"content\"], c[\"source\"], c[\"vector\"]))\n",
    "conn.commit()\n",
    "cur.close()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8838247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "จากข้อมูลที่ให้มา กฏการลางานมีดังนี้:\n",
      "\n",
      "*   **ลาพักร้อน:** ได้ 30 วัน/ปี\n",
      "*   **ลาป่วย:** แจ้งภายในเดือนนั้น และหากป่วย ≥ 2 วัน ต้องมีใบรับรองแพทย์\n",
      "*   **ลาล่วงหน้า:** ลากิจ - แจ้งลาล่วงหน้าอย่างน้อย 3 วัน (ผ่านหัวหน้า + HR)\n",
      "*   **ลา ≥ 2 วัน:** ต้องแจ้งล่วงหน้า 1 สัปดาห์\n",
      "*   **แลก Point:** สามารถแลกเป็น Point ได้ (200 Point/วัน ภายใน 30 พ.ย.)\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "\n",
    "# 2. ฟังก์ชันสืบค้นเอกสารจาก Postgres ตามความใกล้เคียงของเวกเตอร์\n",
    "def retrieve_similar(query: str, top_k: int = 6):\n",
    "    \"\"\"\n",
    "    รับข้อความค้นหา (query) -> แปลงเป็นเวกเตอร์ -> \n",
    "    SELECT เอกสารที่ embedding ใกล้เคียงที่สุด top_k ชิ้น\n",
    "    \"\"\"\n",
    "    # แปลง query เป็นเวกเตอร์\n",
    "    q_vec = embed_model.encode([query], show_progress_bar=False)[0].tolist()\n",
    "\n",
    "    # เชื่อมต่อฐานข้อมูล\n",
    "    conn = psycopg2.connect(\n",
    "        dbname=\"mydb\", user=\"admin\", password=\"1234\",\n",
    "        host=\"localhost\", port=\"5432\"\n",
    "    )\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    # ใช้ operator <-> ของ pgvector เพื่อคำนวณระยะ Euclidean\n",
    "    # ต้องแปลง array เป็น vector ด้วย ::vector\n",
    "    cur.execute(\"\"\"\n",
    "        SELECT content, source\n",
    "        FROM documents\n",
    "        ORDER BY embedding <-> (%s::vector)\n",
    "        LIMIT %s\n",
    "    \"\"\", (q_vec, top_k))\n",
    "\n",
    "    rows = cur.fetchall()\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "    # คืนผลเป็น list ของ dict\n",
    "    return [{\"content\": r[0], \"source\": r[1]} for r in rows]\n",
    "\n",
    "\n",
    "# 3. ฟังก์ชันรวม context และเรียก LLM (ตัวอย่างสำหรับ local API)\n",
    "import requests\n",
    "\n",
    "API_URL = \"http://localhost:5433\"       # แก้เป็น URL ของ LLM service\n",
    "MODEL_NAME = \"scb10x/typhoon2.1-gemma3-4b:latest\"\n",
    "\n",
    "def generate_rag_response(query: str, top_k: int = 6) -> str:\n",
    "    # 3.1 สืบค้นเอกสารที่เกี่ยวข้อง\n",
    "    docs = retrieve_similar(query, top_k)\n",
    "\n",
    "    # 3.2 ประกอบ context\n",
    "    context = \"\\n\\n\".join(\n",
    "        f\"Source: {doc['source']}\\n{doc['content']}\" for doc in docs\n",
    "    )\n",
    "    prompt = (\n",
    "    \"คุณคือผู้ช่วยอัจฉริยะชื่อ TTT-Assistant จากบริษัท ทีทีที บราเธอร์ส จำกัด \"\n",
    "    \"กรุณาตอบคำถามให้ละเอียดและครบถ้วนที่สุด โดยใช้ข้อมูลจาก context ที่มี\\n\"\n",
    "        f\"---\\nContext: {context}\\n---\"\n",
    "        \"\\n\"\n",
    "        f\"Question: {query}\"\n",
    "        \"\\nAnswer:\"\n",
    "    )\n",
    "\n",
    "    # 3.3 เรียก LLM API เพื่อสร้างคำตอบ\n",
    "    res = requests.post(\n",
    "        f\"{API_URL}/api/generate\",\n",
    "        json={\n",
    "            \"model\": MODEL_NAME,\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": False\n",
    "        }\n",
    "    )\n",
    "    res.raise_for_status()\n",
    "    return res.json()[\"response\"]\n",
    "\n",
    "# 4. วิธีใช้งาน\n",
    "if __name__ == \"__main__\":\n",
    "    question = \"กฏการลางานมีอะไรบ้าง\"\n",
    "    answer = generate_rag_response(question, top_k=3)\n",
    "    print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
